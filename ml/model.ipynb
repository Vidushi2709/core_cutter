{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b40a56bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import xgboost as xgb\n",
    "import lightgbm as lgb\n",
    "import catboost as cb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "fe5762d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training data shape: (14000, 4)\n",
      "Test data shape: (14000, 3)\n",
      "\n",
      "Class distribution:\n",
      "switch\n",
      "not_switch    7000\n",
      "switch        7000\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Load training and test data (FIX: train should use training_data.csv)\n",
    "train_df = pd.read_csv('phase_balancing_training_data.csv')\n",
    "test_df = pd.read_csv('phase_balancing_test_data.csv')\n",
    "\n",
    "print(f\"Training data shape: {train_df.shape}\")\n",
    "print(f\"Test data shape: {test_df.shape}\")\n",
    "print(f\"\\nClass distribution:\\n{train_df['switch'].value_counts()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "92c920ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training features: (14000, 3)\n",
      "Test features: (14000, 3)\n",
      "Label encoding: {'not_switch': np.int64(0), 'switch': np.int64(1)}\n"
     ]
    }
   ],
   "source": [
    "# Prepare features and labels\n",
    "X_train = train_df[['L1', 'L2', 'L3']].to_numpy()\n",
    "y_train = train_df['switch'].to_numpy()\n",
    "\n",
    "X_test = test_df[['L1', 'L2', 'L3']].to_numpy()\n",
    "\n",
    "# Encode labels (switch -> 1, not_switch -> 0)\n",
    "le = LabelEncoder()\n",
    "y_train_encoded = le.fit_transform(y_train)\n",
    "\n",
    "print(f\"Training features: {X_train.shape}\")\n",
    "print(f\"Test features: {X_test.shape}\")\n",
    "print(f\"Label encoding: {dict(zip(list(le.classes_), list(le.transform(le.classes_))))}\") # type: ignore"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69a6dff4",
   "metadata": {},
   "source": [
    "## Why XGBoost and LogLoss?\n",
    "\n",
    "**Why XGBoost (Extreme Gradient Boosting)?**\n",
    "- **Ensemble Learning**: Combines multiple weak learners (decision trees) to create a strong predictor\n",
    "- **Handles Imbalanced Data**: Works well with our 50/50 split and can handle class imbalance\n",
    "- **Regularization**: Built-in L1 (Lasso) and L2 (Ridge) regularization prevents overfitting\n",
    "- **Feature Importance**: Provides insights into which features (L1, L2, L3) matter most\n",
    "- **Speed & Efficiency**: Optimized for performance with parallel processing\n",
    "- **Robust to Outliers**: Tree-based models handle extreme values well (important for our export scenarios)\n",
    "\n",
    "**Why LogLoss (Cross-Entropy Loss)?**\n",
    "- **Probabilistic Predictions**: Measures the quality of probability estimates, not just class labels\n",
    "- **Penalizes Confident Wrong Predictions**: High penalty when model is confident but wrong\n",
    "- **Smooth Gradient**: Continuous differentiable function enables gradient descent optimization\n",
    "- **Binary Classification Standard**: Industry standard for binary classification problems like ours (switch/not_switch)\n",
    "- **Better than Accuracy**: Captures prediction certainty, crucial for phase switching decisions where confidence matters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "b36d40c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìä Overfitting Check (Train/Val Split):\n",
      "Training Accuracy: 0.9908 (99.08%)\n",
      "Validation Accuracy: 0.9914 (99.14%)\n",
      "Gap (Train - Val): -0.0006\n",
      "‚úÖ Good generalization (gap < 2%)\n",
      "\n",
      "5-Fold Cross-Validation:\n",
      "XGBoost - CV Accuracy: 0.9866 (+/- 0.0055)\n",
      "Individual fold scores: ['0.9768', '0.9850', '0.9879', '0.9918', '0.9914']\n",
      "\n",
      "‚úÖ Using Anti-Overfitting XGBoost with regularization\n"
     ]
    }
   ],
   "source": [
    "# Enhanced XGBoost with Anti-Overfitting Parameters\n",
    "from sklearn.model_selection import cross_val_score, train_test_split\n",
    "\n",
    "# Split data for validation monitoring\n",
    "X_train_split, X_val_split, y_train_split, y_val_split = train_test_split(\n",
    "    X_train, y_train_encoded, test_size=0.2, random_state=42, stratify=y_train_encoded\n",
    ")\n",
    "\n",
    "# XGBoost model with anti-overfitting parameters\n",
    "xgb_model = xgb.XGBClassifier(\n",
    "    n_estimators=100,           # Reduced from 200 to prevent overfitting\n",
    "    max_depth=4,                # Reduced from 6 - shallower trees generalize better\n",
    "    learning_rate=0.05,         # Reduced from 0.1 - slower learning prevents overfitting\n",
    "    subsample=0.7,              # Reduced from 0.8 - more randomness\n",
    "    colsample_bytree=0.7,       # Reduced from 0.8 - use fewer features per tree\n",
    "    colsample_bylevel=0.7,      # Reduced from 0.8\n",
    "    gamma=1.0,                  # Increased from 0.1 - stronger pruning (minimum loss reduction)\n",
    "    min_child_weight=5,         # Increased from 3 - require more samples in leaf\n",
    "    reg_alpha=1.0,              # Increased from 0.1 - stronger L1 regularization\n",
    "    reg_lambda=2.0,             # Increased from 1.0 - stronger L2 regularization\n",
    "    scale_pos_weight=1,\n",
    "    random_state=42,\n",
    "    eval_metric='logloss',\n",
    "    tree_method='hist',\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "# Fit with validation monitoring (for overfitting check)\n",
    "xgb_model.fit(\n",
    "    X_train_split, y_train_split,\n",
    "    eval_set=[(X_train_split, y_train_split), (X_val_split, y_val_split)],\n",
    "    verbose=False\n",
    ")\n",
    "\n",
    "# Check for overfitting by comparing train vs validation\n",
    "train_pred = xgb_model.predict(X_train_split)\n",
    "val_pred = xgb_model.predict(X_val_split)\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "train_acc = accuracy_score(y_train_split, train_pred)\n",
    "val_acc = accuracy_score(y_val_split, val_pred)\n",
    "\n",
    "print(f\"üìä Overfitting Check (Train/Val Split):\")\n",
    "print(f\"Training Accuracy: {train_acc:.4f} ({train_acc*100:.2f}%)\")\n",
    "print(f\"Validation Accuracy: {val_acc:.4f} ({val_acc*100:.2f}%)\")\n",
    "print(f\"Gap (Train - Val): {(train_acc - val_acc):.4f}\")\n",
    "\n",
    "if train_acc - val_acc > 0.05:\n",
    "    print(\"‚ö†Ô∏è  Warning: Significant overfitting detected (gap > 5%)\")\n",
    "elif train_acc - val_acc > 0.02:\n",
    "    print(\"‚ö†Ô∏è  Mild overfitting detected (gap 2-5%)\")\n",
    "else:\n",
    "    print(\"‚úÖ Good generalization (gap < 2%)\")\n",
    "\n",
    "# Now create fresh model for cross-validation (without early stopping)\n",
    "xgb_model_cv = xgb.XGBClassifier(\n",
    "    n_estimators=100,\n",
    "    max_depth=4,\n",
    "    learning_rate=0.05,\n",
    "    subsample=0.7,\n",
    "    colsample_bytree=0.7,\n",
    "    colsample_bylevel=0.7,\n",
    "    gamma=1.0,\n",
    "    min_child_weight=5,\n",
    "    reg_alpha=1.0,\n",
    "    reg_lambda=2.0,\n",
    "    scale_pos_weight=1,\n",
    "    random_state=42,\n",
    "    eval_metric='logloss',\n",
    "    tree_method='hist',\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "# Cross-validation scores on full training data\n",
    "scores = cross_val_score(xgb_model_cv, X_train, y_train_encoded, cv=5, scoring='accuracy')\n",
    "print(f\"\\n5-Fold Cross-Validation:\")\n",
    "print(f\"XGBoost - CV Accuracy: {scores.mean():.4f} (+/- {scores.std():.4f})\")\n",
    "print(f\"Individual fold scores: {[f'{s:.4f}' for s in scores]}\")\n",
    "\n",
    "# Train final model on full training data\n",
    "xgb_model_cv.fit(X_train, y_train_encoded)\n",
    "\n",
    "# Set as best model\n",
    "best_model_name = 'XGBoost'\n",
    "best_model = xgb_model_cv\n",
    "\n",
    "print(f\"\\n‚úÖ Using Anti-Overfitting XGBoost with regularization\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "5c3b1797",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìä Model Performance Metrics using XGBoost:\n",
      "============================================================\n",
      "\n",
      "üîπ Training Set:\n",
      "   Log Loss: 0.1409\n",
      "   Accuracy: 0.9907 (99.07%)\n",
      "\n",
      "üîπ Test Set Predictions:\n",
      "   Total predictions: 14000\n",
      "   not_switch: 4827 (34.5%)\n",
      "   switch: 9173 (65.5%)\n",
      "\n",
      "üîπ Prediction Confidence (switch):\n",
      "   Mean: 0.9110\n",
      "   Min: 0.5005, Max: 0.9961\n",
      "\n",
      "üîπ Prediction Confidence (not_switch):\n",
      "   Mean: 0.8273\n",
      "   Min: 0.5020, Max: 0.9168\n",
      "\n",
      "üìã First 10 predictions with confidence:\n",
      "     L1    L2    L3 predicted_switch  confidence\n",
      "0  2.55  0.97  5.31           switch    0.896121\n",
      "1 -1.31 -1.38 -1.29           switch    0.996085\n",
      "2  3.74  3.66  3.45       not_switch    0.822054\n",
      "3 -1.71 -1.80 -1.73           switch    0.996085\n",
      "4  5.29  5.35  5.28       not_switch    0.916097\n",
      "5  4.75  0.38  1.90           switch    0.858436\n",
      "6  2.81  2.58  2.99       not_switch    0.819929\n",
      "7  1.02  2.91  3.71           switch    0.754264\n",
      "8  3.70  3.32  3.66       not_switch    0.830484\n",
      "9  4.89  2.08  1.77           switch    0.752867\n",
      "\n",
      "‚úÖ Results saved to test_predictions.csv\n"
     ]
    }
   ],
   "source": [
    "# Train the best model on full training data and predict on test set\n",
    "from sklearn.metrics import log_loss, accuracy_score, classification_report, confusion_matrix\n",
    "\n",
    "best_model.fit(X_train, y_train_encoded)\n",
    "\n",
    "# Predict on test data\n",
    "test_predictions = best_model.predict(X_test)\n",
    "test_predictions_proba = best_model.predict_proba(X_test)  # Get probability estimates\n",
    "test_predictions_labels = le.inverse_transform(test_predictions)\n",
    "\n",
    "# Calculate metrics on training data (for comparison)\n",
    "train_predictions = best_model.predict(X_train)\n",
    "train_predictions_proba = best_model.predict_proba(X_train)\n",
    "\n",
    "train_loss = log_loss(y_train_encoded, train_predictions_proba)\n",
    "train_accuracy = accuracy_score(y_train_encoded, train_predictions)\n",
    "\n",
    "print(f\"üìä Model Performance Metrics using {best_model_name}:\")\n",
    "print(\"=\"*60)\n",
    "print(f\"\\nüîπ Training Set:\")\n",
    "print(f\"   Log Loss: {train_loss:.4f}\")\n",
    "print(f\"   Accuracy: {train_accuracy:.4f} ({train_accuracy*100:.2f}%)\")\n",
    "\n",
    "# Since test data doesn't have labels, we can't calculate test loss\n",
    "# But we can show prediction distribution and confidence\n",
    "print(f\"\\nüîπ Test Set Predictions:\")\n",
    "print(f\"   Total predictions: {len(test_predictions)}\")\n",
    "\n",
    "# Show prediction distribution\n",
    "unique, counts = np.unique(test_predictions_labels, return_counts=True)\n",
    "for label, count in zip(unique, counts):\n",
    "    print(f\"   {label}: {count} ({count/len(test_predictions)*100:.1f}%)\")\n",
    "\n",
    "# Show confidence statistics\n",
    "confidence_switch = test_predictions_proba[test_predictions == 1][:, 1]\n",
    "confidence_not_switch = test_predictions_proba[test_predictions == 0][:, 0]\n",
    "\n",
    "if len(confidence_switch) > 0:\n",
    "    print(f\"\\nüîπ Prediction Confidence (switch):\")\n",
    "    print(f\"   Mean: {confidence_switch.mean():.4f}\")\n",
    "    print(f\"   Min: {confidence_switch.min():.4f}, Max: {confidence_switch.max():.4f}\")\n",
    "\n",
    "if len(confidence_not_switch) > 0:\n",
    "    print(f\"\\nüîπ Prediction Confidence (not_switch):\")\n",
    "    print(f\"   Mean: {confidence_not_switch.mean():.4f}\")\n",
    "    print(f\"   Min: {confidence_not_switch.min():.4f}, Max: {confidence_not_switch.max():.4f}\")\n",
    "\n",
    "# Create results dataframe with confidence scores\n",
    "results_df = test_df.copy()\n",
    "results_df['predicted_switch'] = list(test_predictions_labels)\n",
    "results_df['confidence'] = [test_predictions_proba[i][test_predictions[i]] for i in range(len(test_predictions))]\n",
    "\n",
    "print(f\"\\nüìã First 10 predictions with confidence:\")\n",
    "print(results_df[['L1', 'L2', 'L3', 'predicted_switch', 'confidence']].head(10))\n",
    "\n",
    "# Save results\n",
    "results_df.to_csv('test_predictions.csv', index=False)\n",
    "print(f\"\\n‚úÖ Results saved to test_predictions.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "1d830888",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Model saved to phase_balancing_model.pkl\n",
      "‚úÖ Label encoder saved to label_encoder.pkl\n",
      "\n",
      "‚úÖ Model verification successful!\n",
      "Sample predictions:\n",
      "  L1=2.5, L2=2.6, L3=2.4 ‚Üí not_switch (confidence: 0.7943)\n",
      "  L1=1.0, L2=5.5, L3=2.0 ‚Üí switch (confidence: 0.8790)\n"
     ]
    }
   ],
   "source": [
    "# Save the trained model and label encoder\n",
    "import pickle\n",
    "\n",
    "# Save XGBoost model\n",
    "model_path = 'phase_balancing_model.pkl'\n",
    "with open(model_path, 'wb') as f:\n",
    "    pickle.dump(best_model, f)\n",
    "print(f\"‚úÖ Model saved to {model_path}\")\n",
    "\n",
    "# Save label encoder\n",
    "encoder_path = 'label_encoder.pkl'\n",
    "with open(encoder_path, 'wb') as f:\n",
    "    pickle.dump(le, f)\n",
    "print(f\"‚úÖ Label encoder saved to {encoder_path}\")\n",
    "\n",
    "# Verify saved model by loading and testing\n",
    "with open(model_path, 'rb') as f:\n",
    "    loaded_model = pickle.load(f)\n",
    "\n",
    "with open(encoder_path, 'rb') as f:\n",
    "    loaded_encoder = pickle.load(f)\n",
    "\n",
    "# Test with sample data\n",
    "test_sample = [[2.5, 2.6, 2.4], [1.0, 5.5, 2.0]]\n",
    "test_pred = loaded_model.predict(test_sample)\n",
    "test_pred_labels = loaded_encoder.inverse_transform(test_pred)\n",
    "test_pred_proba = loaded_model.predict_proba(test_sample)\n",
    "\n",
    "print(f\"\\n‚úÖ Model verification successful!\")\n",
    "print(f\"Sample predictions:\")\n",
    "for i, (sample, pred, proba) in enumerate(zip(test_sample, test_pred_labels, test_pred_proba)):\n",
    "    confidence = proba[test_pred[i]]\n",
    "    print(f\"  L1={sample[0]}, L2={sample[1]}, L3={sample[2]} ‚Üí {pred} (confidence: {confidence:.4f})\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
